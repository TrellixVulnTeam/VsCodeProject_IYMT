'''
------------------------------------------------------------------------------------------
text
    Text generation with an RNN

This tutorial demonstrates how to generate text using a character-based RNN. 
We will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. 
Given a sequence of characters from this data ("Shakespear"), train a model to predict the next character in the sequence ("e"). 
Longer sequences of text can be generated by calling the model repeatedly.

Note: 
Enable GPU acceleration to execute this notebook faster. 
In Colab: Runtime > Change runtime type > Hardware acclerator > GPU. 
If running locally make sure TensorFlow version >= 1.11.

This tutorial includes runnable code implemented using tf.keras and eager execution. 
The following is sample output when the model in this tutorial trained for 30 epochs, and started with the string "Q":

    QUEENE:
    I had thought thou hadst a Roman; for the oracle,
    Thus by All bids the man against the word,
    Which are so weak of care, by old care done;
    Your children were in your holy love,
    And the precipitation through the bleeding throne.

    BISHOP OF ELY:
    Marry, and will, my lord, to weep in such a one were prettiest;
    Yet now I was adopted heir
    Of the world's lamentable day,
    To watch the next way with his father with his face?

    ESCALUS:
    The cause why then we are all resolved more sons.

    VOLUMNIA:
    O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,
    And love and pale as any will to that word.

    QUEEN ELIZABETH:
    But how long have I heard the soul for this world,
    And show his hands of life be proved to stand.

    PETRUCHIO:
    I say he look'd on, if I must be content
    To stay him from the fatal of our country's bliss.
    His lordship pluck'd from this sentence then for prey,
    And then let us twain, being the moon,
    were she such a case as fills m

While some of the sentences are grammatical, most do not make sense. 
The model has not learned the meaning of words, but consider:

	* The model is character-based. 
	When training started, the model did not know how to spell an English word, 
	or that words were even a unit of text.

	* The structure of the output resembles a play—blocks of text generally begin with a speaker name, 
	in all capital letters similar to the dataset.

	* As demonstrated below, the model is trained on small batches of text (100 characters each), 
	and is still able to generate a longer sequence of text with coherent structure.
------------------------------------------------------------------------------------------
'''
# common library
from __future__ import absolute_import, division, print_function, unicode_literals

import time
import os
import sys
import io
import pprint
import contextlib
from pathlib import Path
from packaging import version
from PIL import Image
import tempfile

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import tensorflow_datasets as tfds

print(__doc__)


tfds.disable_progress_bar()

AUTOTUNE = tf.data.experimental.AUTOTUNE

pd.options.display.max_rows = None

# Display current path
basic_path = Path.cwd()
PROJECT_ROOT_DIR = basic_path.joinpath('Python/Normal/tensorflow')
print('PROJECT_ROOT_DIR = \n{0}\n'.format(PROJECT_ROOT_DIR))

# Display tensorflow version
print("TensorFlow version: ", tf.version.VERSION)
assert version.parse(tf.version.VERSION).release[0] >= 2, \
"This notebook requires TensorFlow 2.0 or above."

print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       Download the Shakespeare dataset                                                               \n'
        '------------------------------------------------------------------------------------------------------\n'
        )
'''
---------------------------------------------------------------------------------------------------------------
Change the following line to run this code on your own data.
--------------------------------------------------------------------------------------------------------------
'''
path_to_file = tf.keras.utils.get_file(
                    origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',
                    fname=PROJECT_ROOT_DIR.joinpath('Data/shakeSpeare/shakespeare.txt')
                )

print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       Read the data                                                                                  \n'
        '------------------------------------------------------------------------------------------------------\n'
        )

# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))

# Take a look at the first 250 characters in text
print(text[:250])

# The unique characters in the file
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))

print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       Process the text                                                                               \n'
        '------------------------------------------------------------------------------------------------------\n'
        )
print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       Vectorize the text                                                                             \n'
        '------------------------------------------------------------------------------------------------------\n'
        )
'''
----------------------------------------------------------------------------------------------------------------
Before training, we need to map strings to a numerical representation. 
Create two lookup tables: one mapping characters to numbers, and another for numbers to characters.
----------------------------------------------------------------------------------------------------------------
'''
# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

'''
----------------------------------------------------------------------------------------------------------------
Now we have an integer representation for each character. 
Notice that we mapped the character as indexes from 0 to len(unique).
----------------------------------------------------------------------------------------------------------------
'''
print('{')
for char,_ in zip(char2idx, range(20)):
    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))
print('  ...\n}')

# Show how the first 13 characters from the text are mapped to integers
print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))

print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       The prediction task                                                                             \n'
        '------------------------------------------------------------------------------------------------------\n'
        )
'''
---------------------------------------------------------------------------------------------------------------
Given a character, or a sequence of characters, what is the most probable next character? 
This is the task we're training the model to perform. 
The input to the model will be a sequence of characters, 
and we train the model to predict the output—the following character at each time step.

Since RNNs maintain an internal state that depends on the previously seen elements, 
given all the characters computed until this moment, what is the next character?
---------------------------------------------------------------------------------------------------------------
'''
print   (
        '------------------------------------------------------------------------------------------------------\n'
        '       Create training examples and targets                                                           \n'
        '------------------------------------------------------------------------------------------------------\n'
        )
'''
----------------------------------------------------------------------------------------------------------------
Next divide the text into example sequences. 
Each input sequence will contain seq_length characters from the text.

For each input sequence, the corresponding targets contain the same length of text, 
except shifted one character to the right.

So break the text into chunks of seq_length+1. 
For example, say seq_length is 4 and our text is "Hello". 
The input sequence would be "Hell", and the target sequence "ello".

To do this first use the tf.data.Dataset.from_tensor_slices function 
to convert the text vector into a stream of character indices.
-----------------------------------------------------------------------------------------------------------------
'''
